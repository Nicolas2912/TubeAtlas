from __future__ import annotations

import hashlib
import json
import re
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Sequence, Tuple

import networkx as nx
from networkx.readwrite import json_graph

import numpy as np  # type: ignore

try:
    import faiss  # type: ignore
except ModuleNotFoundError:  # pragma: no cover – optional dependency
    faiss = None  # type: ignore

from .embeddings import embed_text

# ---------------------------------------------------------------------------
# Data Structures
# ---------------------------------------------------------------------------


@dataclass(slots=True)
class Provenance:  # noqa: D101 (docstring autogenerated below)
    """Metadata describing the origin of a triple produced by the LLM pipeline."""

    chunk_id: str
    video_id: str
    channel_id: str
    llm_model: str
    t_created: datetime

    def to_dict(self) -> Dict[str, str]:  # noqa: D401
        """Serialise to a JSON-friendly dict."""
        return {
            "chunk_id": self.chunk_id,
            "video_id": self.video_id,
            "channel_id": self.channel_id,
            "llm_model": self.llm_model,
            "t_created": self.t_created.isoformat(),
        }


@dataclass(slots=True)
class Triple:  # noqa: D101
    """Subject–predicate–object fact with confidence & provenance."""

    subject: str
    predicate: str
    object: str
    confidence: float  # 0.0-1.0
    provenance: Provenance

    # The canonical signature is cached on first access for speed.
    _signature: str | None = None

    @property
    def signature(self) -> str:  # noqa: D401
        """Return a stable 16-char hash uniquely identifying the triple."""
        if self._signature is None:
            self._signature = canonical_key(self.subject, self.predicate, self.object)
        return self._signature


@dataclass(slots=True)
class MergeStats:  # noqa: D101
    inserted: int = 0
    exact_dupes: int = 0
    fuzzy_dupes: int = 0
    updated: int = 0

    def __iadd__(self, other: "MergeStats") -> "MergeStats":  # noqa: D401
        """In-place addition of counters."""
        self.inserted += other.inserted
        self.exact_dupes += other.exact_dupes
        self.fuzzy_dupes += other.fuzzy_dupes
        self.updated += other.updated
        return self


# ---------------------------------------------------------------------------
# Helper functions
# ---------------------------------------------------------------------------

_WS_RE = re.compile(r"\s+")
_PUNCT_RE = re.compile(r"[\.,;:!\?\-\(\)\[\]'\"]+")


def _normalise(text: str) -> str:
    """Lower-case, strip punctuation, collapse whitespace."""
    text = text.lower().strip()
    text = _PUNCT_RE.sub("", text)
    text = _WS_RE.sub(" ", text)
    return text


def canonical_key(subject: str, predicate: str, obj: str) -> str:  # noqa: D401
    """Return 16-char SHA-256 hash for the triple."""
    raw = f"{_normalise(subject)}|{_normalise(predicate)}|{_normalise(obj)}"
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()[:16]


# ---------------------------------------------------------------------------
# Triple Merger
# ---------------------------------------------------------------------------


class TripleMerger:  # noqa: D101
    def __init__(self, kg_path: Path, sim_thr: float = 0.92) -> None:  # noqa: D401
        """Create a merger bound to the graph file on *kg_path*.

        Only *exact* duplicate detection is implemented in the first iteration.  The
        *sim_thr* parameter is reserved for future fuzzy-matching support.
        """
        self.kg_path = kg_path
        self.sim_thr = sim_thr

        self.graph: nx.MultiDiGraph = self._load_graph()
        # signature -> edge (u, v, key)
        self._sig_idx: Dict[str, Tuple[str, str, int]] = {}
        self._build_signature_index()

        # Embedding store: parallel lists for fallback & mapping idx→signature
        self._embeddings: List[np.ndarray] = []
        self._embedding_sigs: List[str] = []
        self._faiss_index = None
        self._build_embeddings_index()

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    def merge_batch(self, triples: Sequence[Triple]) -> MergeStats:  # noqa: D401
        """Insert *triples* into the KG, returning merge statistics."""
        stats = MergeStats()

        for t in triples:
            sig = t.signature

            if sig in self._sig_idx:  # exact dup
                self._update_edge_attrs(sig, t)
                stats.exact_dupes += 1
                continue

            # ---------- fuzzy duplicate check ----------
            emb = embed_text(f"{t.subject} {t.predicate} {t.object}")
            dup_sig = self._find_fuzzy_duplicate(emb)
            if dup_sig:
                self._update_edge_attrs(dup_sig, t)
                stats.fuzzy_dupes += 1
                continue

            # brand-new triple
            self._add_edge(t, emb)
            stats.inserted += 1

        # Persist changes eagerly for now.
        self._persist()
        return stats

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------

    def _update_edge_attrs(self, sig: str, t: Triple) -> None:  # noqa: D401
        """Refresh existing edge attributes when a duplicate is observed."""
        u, v, key = self._sig_idx[sig]
        # MultiDiGraph edge keys can be int; ignore typing complaints.
        edge_data: Dict[str, Any] = self.graph[u][v][key]  # type: ignore[index]

        # Update frequency counter
        edge_data["frequency"] = int(edge_data.get("frequency", 1)) + 1

        # Re-compute confidence using harmonic mean for stability
        prev_conf: float = float(edge_data.get("confidence", 0.0))
        new_conf = (2 * prev_conf * t.confidence) / (prev_conf + t.confidence) if prev_conf else t.confidence
        edge_data["confidence"] = round(new_conf, 4)

        # Append provenance (keep last 10)
        sources: List[Dict[str, str]] = edge_data.get("sources", [])
        sources.append(t.provenance.to_dict())
        edge_data["sources"] = sources[-10:]
        edge_data["last_seen"] = datetime.now(tz=timezone.utc).isoformat()

    def _add_edge(self, t: Triple, emb: np.ndarray | None = None) -> None:  # noqa: D401
        """Add a brand-new triple as edge with initial attributes."""
        # Ensure nodes exist (NetworkX adds automatically on add_edge).
        edge_key = self.graph.add_edge(
            t.subject,
            t.object,
            predicate=t.predicate,
            confidence=round(t.confidence, 4),
            frequency=1,
            sources=[t.provenance.to_dict()],
            signature=t.signature,
            first_seen=datetime.now(tz=timezone.utc).isoformat(),
            last_seen=datetime.now(tz=timezone.utc).isoformat(),
        )
        # Store signature index mapping
        self._sig_idx[t.signature] = (t.subject, t.object, edge_key)

        # store embedding
        if emb is None:
            emb = embed_text(f"{t.subject} {t.predicate} {t.object}")

        # At this point emb is guaranteed to be ndarray
        self._embeddings.append(emb)
        self._embedding_sigs.append(t.signature)
        if self._faiss_index is not None:
            self._faiss_index.add(emb.reshape(1, -1))  # type: ignore[arg-type]

    # ------------------------------------------------------------------
    # Persistence helpers
    # ------------------------------------------------------------------

    def _load_graph(self) -> nx.MultiDiGraph:  # noqa: D401
        if self.kg_path.exists():
            try:
                return json_graph.node_link_graph(json.load(self.kg_path.open()), directed=True, multigraph=True)
            except Exception:  # pragma: no cover
                # Corrupt file → start fresh but don't crash the pipeline.
                return nx.MultiDiGraph()
        return nx.MultiDiGraph()

    def _persist(self) -> None:  # noqa: D401
        """Write the graph back to disk atomically."""
        tmp_path = self.kg_path.with_suffix(".tmp")
        data = json_graph.node_link_data(self.graph)
        tmp_path.write_text(json.dumps(data, ensure_ascii=False, indent=2))
        tmp_path.replace(self.kg_path)

    # ------------------------------------------------------------------
    # Index helpers
    # ------------------------------------------------------------------

    def _build_signature_index(self) -> None:  # noqa: D401
        """Populate in-memory signature → edge mapping for O(1) look-ups."""
        for u, v, key, data in self.graph.edges(keys=True, data=True):
            sig = data.get("signature")
            if sig:
                self._sig_idx[sig] = (u, v, key)

    # ------------------------------------------------------------------
    # Embedding / fuzzy helpers
    # ------------------------------------------------------------------

    def _build_embeddings_index(self) -> None:  # noqa: D401
        for _, _, _, data in self.graph.edges(keys=True, data=True):
            sig = data.get("signature")
            if not sig:
                continue
            # Build from stored raw string if present else skip
            subj = data.get("source", "")
            pred = data.get("predicate", "")
            obj = data.get("target", "")
            emb = embed_text(f"{subj} {pred} {obj}")
            self._embeddings.append(emb)
            self._embedding_sigs.append(sig)

        if faiss is not None and self._embeddings:
            dim = self._embeddings[0].shape[0]
            self._faiss_index = faiss.IndexFlatIP(dim)
            mat = np.stack(self._embeddings).astype(np.float32)
            self._faiss_index.add(mat)

    def _find_fuzzy_duplicate(self, emb: np.ndarray) -> str | None:  # noqa: D401
        if not self._embeddings:
            return None

        if self._faiss_index is not None:
            emb = emb.astype(np.float32).reshape(1, -1)
            scores, idxs = self._faiss_index.search(emb, k=1)
            best_score = float(scores[0][0])
            if best_score >= self.sim_thr:
                return self._embedding_sigs[int(idxs[0][0])]
            return None

        # Fallback: brute cosine similarity
        best_sim = -1.0
        best_sig = None
        for sig, e in zip(self._embedding_sigs, self._embeddings, strict=False):
            sim = float(e @ emb / (np.linalg.norm(e) * np.linalg.norm(emb)))
            if sim > best_sim:
                best_sim = sim
                best_sig = sig
        return best_sig if best_sim >= self.sim_thr else None