from __future__ import annotations

import hashlib
import json
import re
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Sequence, Tuple

import networkx as nx
from networkx.readwrite import json_graph

# ---------------------------------------------------------------------------
# Data Structures
# ---------------------------------------------------------------------------


@dataclass(slots=True)
class Provenance:  # noqa: D101 (docstring autogenerated below)
    """Metadata describing the origin of a triple produced by the LLM pipeline."""

    chunk_id: str
    video_id: str
    channel_id: str
    llm_model: str
    t_created: datetime

    def to_dict(self) -> Dict[str, str]:  # noqa: D401
        """Serialise to a JSON-friendly dict."""
        return {
            "chunk_id": self.chunk_id,
            "video_id": self.video_id,
            "channel_id": self.channel_id,
            "llm_model": self.llm_model,
            "t_created": self.t_created.isoformat(),
        }


@dataclass(slots=True)
class Triple:  # noqa: D101
    """Subject–predicate–object fact with confidence & provenance."""

    subject: str
    predicate: str
    object: str
    confidence: float  # 0.0-1.0
    provenance: Provenance

    # The canonical signature is cached on first access for speed.
    _signature: str | None = None

    @property
    def signature(self) -> str:  # noqa: D401
        """Return a stable 16-char hash uniquely identifying the triple."""
        if self._signature is None:
            self._signature = canonical_key(self.subject, self.predicate, self.object)
        return self._signature


@dataclass(slots=True)
class MergeStats:  # noqa: D101
    inserted: int = 0
    exact_dupes: int = 0
    fuzzy_dupes: int = 0
    updated: int = 0

    def __iadd__(self, other: "MergeStats") -> "MergeStats":  # noqa: D401
        """In-place addition of counters."""
        self.inserted += other.inserted
        self.exact_dupes += other.exact_dupes
        self.fuzzy_dupes += other.fuzzy_dupes
        self.updated += other.updated
        return self


# ---------------------------------------------------------------------------
# Helper functions
# ---------------------------------------------------------------------------

_WS_RE = re.compile(r"\s+")
_PUNCT_RE = re.compile(r"[\.,;:!\?\-\(\)\[\]'\"]+")


def _normalise(text: str) -> str:
    """Lower-case, strip punctuation, collapse whitespace."""
    text = text.lower().strip()
    text = _PUNCT_RE.sub("", text)
    text = _WS_RE.sub(" ", text)
    return text


def canonical_key(subject: str, predicate: str, obj: str) -> str:  # noqa: D401
    """Return 16-char SHA-256 hash for the triple."""
    raw = f"{_normalise(subject)}|{_normalise(predicate)}|{_normalise(obj)}"
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()[:16]


# ---------------------------------------------------------------------------
# Triple Merger
# ---------------------------------------------------------------------------


class TripleMerger:  # noqa: D101
    def __init__(self, kg_path: Path, sim_thr: float = 0.92) -> None:  # noqa: D401
        """Create a merger bound to the graph file on *kg_path*.

        Only *exact* duplicate detection is implemented in the first iteration.  The
        *sim_thr* parameter is reserved for future fuzzy-matching support.
        """
        self.kg_path = kg_path
        self.sim_thr = sim_thr

        self.graph: nx.MultiDiGraph = self._load_graph()
        # signature -> edge (u, v, key)
        self._sig_idx: Dict[str, Tuple[str, str, int]] = {}
        self._build_signature_index()

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    def merge_batch(self, triples: Sequence[Triple]) -> MergeStats:  # noqa: D401
        """Insert *triples* into the KG, returning merge statistics."""
        stats = MergeStats()

        for t in triples:
            sig = t.signature
            if sig in self._sig_idx:  # exact dup
                self._update_edge_attrs(sig, t)
                stats.exact_dupes += 1
                continue

            self._add_edge(t)
            stats.inserted += 1

        # Persist changes eagerly for now.
        self._persist()
        return stats

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------

    def _update_edge_attrs(self, sig: str, t: Triple) -> None:  # noqa: D401
        """Refresh existing edge attributes when a duplicate is observed."""
        u, v, key = self._sig_idx[sig]
        # MultiDiGraph edge keys can be int; ignore typing complaints.
        edge_data: Dict[str, Any] = self.graph[u][v][key]  # type: ignore[index]

        # Update frequency counter
        edge_data["frequency"] = int(edge_data.get("frequency", 1)) + 1

        # Re-compute confidence using harmonic mean for stability
        prev_conf: float = float(edge_data.get("confidence", 0.0))
        new_conf = (2 * prev_conf * t.confidence) / (prev_conf + t.confidence) if prev_conf else t.confidence
        edge_data["confidence"] = round(new_conf, 4)

        # Append provenance (keep last 10)
        sources: List[Dict[str, str]] = edge_data.get("sources", [])
        sources.append(t.provenance.to_dict())
        edge_data["sources"] = sources[-10:]
        edge_data["last_seen"] = datetime.now(tz=timezone.utc).isoformat()

    def _add_edge(self, t: Triple) -> None:  # noqa: D401
        """Add a brand-new triple as edge with initial attributes."""
        # Ensure nodes exist (NetworkX adds automatically on add_edge).
        edge_key = self.graph.add_edge(
            t.subject,
            t.object,
            predicate=t.predicate,
            confidence=round(t.confidence, 4),
            frequency=1,
            sources=[t.provenance.to_dict()],
            signature=t.signature,
            first_seen=datetime.now(tz=timezone.utc).isoformat(),
            last_seen=datetime.now(tz=timezone.utc).isoformat(),
        )
        # Store signature index mapping
        self._sig_idx[t.signature] = (t.subject, t.object, edge_key)

    # ------------------------------------------------------------------
    # Persistence helpers
    # ------------------------------------------------------------------

    def _load_graph(self) -> nx.MultiDiGraph:  # noqa: D401
        if self.kg_path.exists():
            try:
                return json_graph.node_link_graph(json.load(self.kg_path.open()), directed=True, multigraph=True)
            except Exception:  # pragma: no cover
                # Corrupt file → start fresh but don't crash the pipeline.
                return nx.MultiDiGraph()
        return nx.MultiDiGraph()

    def _persist(self) -> None:  # noqa: D401
        """Write the graph back to disk atomically."""
        tmp_path = self.kg_path.with_suffix(".tmp")
        data = json_graph.node_link_data(self.graph)
        tmp_path.write_text(json.dumps(data, ensure_ascii=False, indent=2))
        tmp_path.replace(self.kg_path)

    # ------------------------------------------------------------------
    # Index helpers
    # ------------------------------------------------------------------

    def _build_signature_index(self) -> None:  # noqa: D401
        """Populate in-memory signature → edge mapping for O(1) look-ups."""
        for u, v, key, data in self.graph.edges(keys=True, data=True):
            sig = data.get("signature")
            if sig:
                self._sig_idx[sig] = (u, v, key)